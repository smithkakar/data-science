{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification - 101 (using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_cifar10_batch(file): \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo, encoding='latin1') \n",
    "    return dict['data'].reshape(-1, 32, 32, 3), dict['labels'] # reshaping the data to 32 x 32 x 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    }
   ],
   "source": [
    "print('Loading...') \n",
    "batch_fns = [os.path.join(\"../data/\", 'cifar-10-batches-py', 'data_batch_' + str(i)) for i in range(1, 6)] \n",
    "data_batches = [_load_cifar10_batch(fn) for fn in batch_fns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.vstack([data_batches[i][0] for i in range(len(data_batches))]).astype('float') \n",
    "labels_all = np.vstack([data_batches[i][1] for i in range(len(data_batches))]).flatten() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Generation\n",
    "\n",
    "We are going to use only a subset of CIFAR-10 dataset.\n",
    "\n",
    "The dataset with 50,000 samples is split in the ratio 92:8. This split is done to take a smaller portion of 50000 samples (i.e the 8% contains only 4000 images).\n",
    "\n",
    "These 4000 samples are used for generating the train and test sets for classification.\n",
    "\n",
    "Here, **StratifiedShuffleSplit** is used to split the dataset. It splits the data by taking equal number of samples from each class in a random manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=7, test_size=0.08,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the whole training set into 92:8\n",
    "seed=7\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Creating data_split object with 8% test size \n",
    "# data_split = StratifiedShuffleSplit(data_all, 1, test_size=0.08, random_state=seed) \n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.08, random_state=seed)\n",
    "sss.get_n_splits(data_all, labels_all)\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in  sss.split(data_all, labels_all):\n",
    "    split_data_92, split_data_8 = data_all[train_index], data_all[test_index]        \n",
    "    split_label_92, split_label_8 = labels_all[train_index], labels_all[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4000 samples are split in the ratio 7:3. (i.e., 2800 for training and 1200 for testing) using StratifiedShuffleSplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training set into 70 and 30\n",
    "# test_size=0.3 denotes that 30% of the dataset is used for testing.\n",
    "# train_test_split = StratifiedShuffleSplit(split_label_8, 1, test_size=0.3,random_state=seed) \n",
    "split8 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=seed)\n",
    "split8.get_n_splits(split_data_8, split_label_8)\n",
    "for train_index, test_index in split8.split(split_data_8, split_label_8):\n",
    "    train_data_70, test_data_30 = split_data_8[train_index], split_data_8[test_index]     \n",
    "    train_label_70, test_label_30 = split_label_8[train_index], split_label_8[test_index]\n",
    "train_data = train_data_70 # assigning to variable train_data\n",
    "train_labels = train_label_70 # assigning to variable train_labels\n",
    "test_data = test_data_30\n",
    "test_labels = test_label_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  (2800, 32, 32, 3)\n",
      "train_labels:  (2800,)\n",
      "test_data:  (1200, 32, 32, 3)\n",
      "test_labels:  (1200,)\n"
     ]
    }
   ],
   "source": [
    "print('train_data: ', train_data.shape)\n",
    "print('train_labels: ', train_labels.shape)\n",
    "print('test_data: ', test_data.shape)\n",
    "print('test_labels: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need for Preprocessing\n",
    "\n",
    "Using the Data preprocessing step, the raw data is converted into a form suitable for subsequent analysis. All the steps before data training (model creation) can be considered as a pre-processing step.\n",
    "\n",
    "The quality of an image is greatly influenced by its clarity and the device used to capture it.\n",
    "\n",
    "The captured image may contain noise and irregularities, which can be removed via preprocessing steps.\n",
    "\n",
    "Some of the common preprocessing techniques include:\n",
    "\n",
    "- Normalization\n",
    "- Dimensionality reduction (eg. PCA, SVD)\n",
    "- Feature Extraction (e.g. SIFT, HOG)\n",
    "- Whitening\n",
    "- Denoising\n",
    "- Contrast Stretching\n",
    "- Background subtraction\n",
    "- Image Enhancement\n",
    "- Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Normalization is the process of converting the pixel intensity values to a normal state.\n",
    "\n",
    "It follows a normal distribution.\n",
    "\n",
    "A normalized image has mean = 0 and variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of normalization function\n",
    "def normalize(data, eps=1e-8): \n",
    "    data -= data.mean(axis=(1, 2, 3), keepdims=True) \n",
    "    std = np.sqrt(data.var(axis=(1, 2, 3), ddof=1, keepdims=True)) # calculating standard deviation\n",
    "    std[std < eps] = 1. \n",
    "    data /= std \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  (2800, 32, 32, 3)\n",
      "test_data:  (1200, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calling the function\n",
    "train_data = normalize(train_data) \n",
    "test_data = normalize(test_data) \n",
    "# prints the shape of train data and test data \n",
    "print('train_data: ', train_data.shape)\n",
    "print('test_data: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZCA Whitening\n",
    "\n",
    "Normalization is followed by a ZCA whitening process.\n",
    "\n",
    "The main aim of whitening is to reduce data redundancy, which means the features are less correlated and have the same variance.\n",
    "\n",
    "ZCA stands for zero-phase component analysis. ZCA whitened images resemble the normal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_flat:  (3072, 2800)\n",
      "test_data_flat:  (3072, 1200)\n"
     ]
    }
   ],
   "source": [
    "# Computing whitening matrix \n",
    "train_data_flat = train_data.reshape(train_data.shape[0], -1).T\n",
    "test_data_flat = test_data.reshape(test_data.shape[0], -1).T\n",
    "print('train_data_flat: ', train_data_flat.shape)\n",
    "print('test_data_flat: ', test_data_flat.shape)\n",
    "train_data_flat_t = train_data_flat.T\n",
    "test_data_flat_t = test_data_flat.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle Component Analysis (PCA)\n",
    "\n",
    "The major function of PCA is to decompose a multivariate dataset into a set of successive orthogonal components. These orthogonal components explain a maximum amount of the variance.\n",
    "\n",
    "PCA is a dimensionality reduction technique.\n",
    "\n",
    "The whitened data is given as the input to PCA.\n",
    "\n",
    "To explore more on PCA, refer this https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# n_components specify the no.of components to keep\n",
    "train_data_pca = PCA(n_components=train_data_flat.shape[1]).fit_transform(train_data_flat)\n",
    "test_data_pca = PCA(n_components=test_data_flat.shape[1]).fit_transform(test_data_flat)\n",
    "train_data_pca = train_data_pca.T\n",
    "test_data_pca = test_data_pca.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a dimensionality reduction technique that has been used in several fields such as image compression, face recognition, and noise filtering.\n",
    "\n",
    "In this method, a digital image (generally considered as a matrix) is decomposed into three other matrices.\n",
    "\n",
    "The singular values (less in number) obtained from this refactoring process can preserve useful features of the original image without utilizing high storage space in the memory.\n",
    "\n",
    "For further details, click http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "# Definition for SVD\n",
    "def svdFeatures(input_data):\n",
    "    svdArray_input_data = []\n",
    "    size = input_data.shape[0]\n",
    "    for i in range (0, size):\n",
    "        img = color.rgb2gray(input_data[i])\n",
    "        U, s, V = np.linalg.svd(img, full_matrices=False);\n",
    "        S = [s[i] for i in range(30)]\n",
    "        svdArray_input_data.append(S)\n",
    "        svdMatrix_input_data = np.matrix(svdArray_input_data)\n",
    "    return svdMatrix_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD for train and test data\n",
    "train_data_svd=svdFeatures(train_data)\n",
    "test_data_svd=svdFeatures(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale-Invariant Feature Transform for Feature Generation (SIFT)\n",
    "\n",
    "SIFT is mainly used for images that are less simple and less organized.\n",
    "\n",
    "Even the photographs of the same material will undergo scale change corresponding to the distance from the material, focal length etc. This is one of the reasons for not considering the raw pixel values as useful features for images.\n",
    "\n",
    "The main aim of using SIFT for feature extraction is to obtain features that are not sensitive to changes in scale, rotation, image resolution, illumination, etc.\n",
    "\n",
    "The major steps involved in SIFT algorithm are:\n",
    "\n",
    "- Scale-space Extrema Detection\n",
    "- Keypoint Localization\n",
    "- Orientation Assignment\n",
    "- Keypoint Descriptor\n",
    "\n",
    "For further details, refer https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building\n",
    "\n",
    "Here, train_data_flat_t can be replaced with train_data_pca or train_data_svd for PCA and SVD respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma=0.001, probability=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm # creating a svm classifier model\n",
    "clf = svm.SVC(gamma=.001,probability=True) # model training\n",
    "clf.fit(train_data_flat_t, train_labels) # after being fitted, the model can then be used to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.38666666666666666\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(test_data_flat_t)\n",
    "score = clf.score(test_data_flat_t,test_labels) # classification score.\n",
    "print(\"score\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, test_data_flat_t can be replaced with test_data_pca or test_data_svd.\n",
    "\n",
    "Above mentioned conventional classification algorithms could not give significant accuracy. But, a better performance can be achieved by using deep learning techniques like **Convolutional Neural Networks (CNN)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)\n",
    "\n",
    "Deep learning has become more important for learning complex algorithms. It is a more refined form of machine learning, which is based on neural networks that emulate the brain.\n",
    "\n",
    "Neural network consists of:\n",
    "\n",
    "- input layer\n",
    "- hidden layers\n",
    "- output layer\n",
    "\n",
    "Each layer is composed of nodes, where the computation happens.\n",
    "\n",
    "Neural Network consists of interconnected neurons that passes messages between each other.\n",
    "\n",
    "CNN is a special case of neural networks that consists of multiple convolutional layers, pooling layers and finally, fully connected layers.\n",
    "\n",
    "The improved network structure helps in saving memory and computational complexity. They are mainly used in pattern and image recognition problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: [[47 13 11  4  1  6  4  5 20  9]\n",
      " [ 2 60  4 11  9  7  4  5  7 11]\n",
      " [15  8 31 14 15 11  9  7  8  2]\n",
      " [ 3  4 10 37 11 27 12  9  3  4]\n",
      " [ 7  4 16  8 30 10 19 11  7  8]\n",
      " [ 1  4 13 24  9 43 17  6  1  2]\n",
      " [ 0  6 18 17 17 11 43  5  0  3]\n",
      " [ 4  2  5 11 17  9  8 48  1 15]\n",
      " [10 14  1  5  2  6  1  2 62 17]\n",
      " [ 3 22  3  6  0  4  5  3 11 63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "conf_matrix = metrics.confusion_matrix(test_labels, predicted)\n",
    "print(\"Confusion matrix:\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-wise accuracy\n",
    "CA = (Correctly predicted images of a class / (Total images of the class)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.16666663402778, 49.99999995833333, 25.833333311805557, 30.83333330763889, 24.999999979166667, 35.83333330347222, 35.83333330347222, 39.999999966666664, 51.666666623611114, 52.49999995625]\n"
     ]
    }
   ],
   "source": [
    "# To see the accuracy of each class. \n",
    "accuracy = []\n",
    "leng = len(conf_matrix) # finding the length of confusion matrix\n",
    "for i in range(leng): \n",
    "# Each diagonal element (conf_matrix[i,i]) is divided by the sum of the elements of that particular row \n",
    "# (conf_matrix[i].sum()).\n",
    "    ac = (conf_matrix[i,i] / ((conf_matrix[i].sum()) + .0000001)) * 100 \n",
    "    accuracy.append(ac)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.66666663444444\n"
     ]
    }
   ],
   "source": [
    "# Overall accuracy is given by, OA = Sum of class-wise accuracy/no of classes\n",
    "summation = 0\n",
    "no_of_classes = 10\n",
    "for i in range(0,len(accuracy)):\n",
    "    summation += accuracy[i]\n",
    "\n",
    "overall_accuracy = summation / no_of_classes\n",
    "print(overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
